{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "75SoflPBVwV1"
   },
   "outputs": [],
   "source": [
    "# Convolutional Neural Network\n",
    "\n",
    "# Importing the libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "YKMnjBHyXJMR",
    "outputId": "11e70d29-1852-49c1-90f8-4b3d68b6975d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "-MA2LfKkVwWF",
    "outputId": "cc21bc91-b64f-464d-9458-de9d76ea392b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1951 images belonging to 4 classes.\n",
      "Found 18 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# Part 1 - Data Preprocessing\n",
    "\n",
    "# Preprocessing the Training set\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('/content/drive/My Drive/AML/Cottondisease/data/train',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "# Preprocessing the Test set\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('/content/drive/My Drive/AML/Cottondisease/data/test',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nkICJGlCVwWU"
   },
   "outputs": [],
   "source": [
    "# Part 2 - Building the CNN\n",
    "\n",
    "# Initialising the CNN\n",
    "cnn = tf.keras.models.Sequential()\n",
    "\n",
    "# Step 1 - Convolution\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))\n",
    "\n",
    "# Step 2 - Pooling\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))\n",
    "\n",
    "# Step 3 - Flattening\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Step 4 - Full Connection\n",
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Step 5 - Output Layer\n",
    "cnn.add(tf.keras.layers.Dense(units=4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "hb6vHafpVwWg",
    "outputId": "a4006361-31dd-474b-a082-fe238baaa6d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 813,604\n",
      "Trainable params: 813,604\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 904
    },
    "id": "GLWEis3eVwWr",
    "outputId": "fdaaaf02-18c8-48a7-f5f8-eb03c6d3bec7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "61/61 [==============================] - 503s 8s/step - loss: 1.1894 - accuracy: 0.4885 - val_loss: 0.7908 - val_accuracy: 0.6111\n",
      "Epoch 2/25\n",
      "61/61 [==============================] - 17s 278ms/step - loss: 0.9056 - accuracy: 0.6540 - val_loss: 0.7929 - val_accuracy: 0.7778\n",
      "Epoch 3/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.7511 - accuracy: 0.7094 - val_loss: 0.8168 - val_accuracy: 0.6667\n",
      "Epoch 4/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.6861 - accuracy: 0.7324 - val_loss: 0.5846 - val_accuracy: 0.8333\n",
      "Epoch 5/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.6384 - accuracy: 0.7617 - val_loss: 0.7414 - val_accuracy: 0.7778\n",
      "Epoch 6/25\n",
      "61/61 [==============================] - 17s 278ms/step - loss: 0.6125 - accuracy: 0.7678 - val_loss: 0.5402 - val_accuracy: 0.8889\n",
      "Epoch 7/25\n",
      "61/61 [==============================] - 17s 277ms/step - loss: 0.5335 - accuracy: 0.7878 - val_loss: 0.7585 - val_accuracy: 0.7778\n",
      "Epoch 8/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.5112 - accuracy: 0.8027 - val_loss: 0.4576 - val_accuracy: 0.8333\n",
      "Epoch 9/25\n",
      "61/61 [==============================] - 17s 280ms/step - loss: 0.5052 - accuracy: 0.7986 - val_loss: 0.4347 - val_accuracy: 0.8889\n",
      "Epoch 10/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.4412 - accuracy: 0.8360 - val_loss: 0.5216 - val_accuracy: 0.8333\n",
      "Epoch 11/25\n",
      "61/61 [==============================] - 17s 278ms/step - loss: 0.4505 - accuracy: 0.8380 - val_loss: 0.2484 - val_accuracy: 0.9444\n",
      "Epoch 12/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.4366 - accuracy: 0.8329 - val_loss: 0.5626 - val_accuracy: 0.7222\n",
      "Epoch 13/25\n",
      "61/61 [==============================] - 17s 279ms/step - loss: 0.4286 - accuracy: 0.8421 - val_loss: 0.2543 - val_accuracy: 0.8889\n",
      "Epoch 14/25\n",
      "61/61 [==============================] - 17s 282ms/step - loss: 0.3906 - accuracy: 0.8503 - val_loss: 0.4311 - val_accuracy: 0.8889\n",
      "Epoch 15/25\n",
      "61/61 [==============================] - 18s 288ms/step - loss: 0.3764 - accuracy: 0.8539 - val_loss: 0.3206 - val_accuracy: 0.8889\n",
      "Epoch 16/25\n",
      "61/61 [==============================] - 17s 285ms/step - loss: 0.3395 - accuracy: 0.8672 - val_loss: 0.1221 - val_accuracy: 0.9444\n",
      "Epoch 17/25\n",
      "61/61 [==============================] - 17s 284ms/step - loss: 0.3425 - accuracy: 0.8652 - val_loss: 0.6221 - val_accuracy: 0.8889\n",
      "Epoch 18/25\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.3252 - accuracy: 0.8785 - val_loss: 0.2950 - val_accuracy: 0.8889\n",
      "Epoch 19/25\n",
      "61/61 [==============================] - 17s 285ms/step - loss: 0.3055 - accuracy: 0.8842 - val_loss: 0.1432 - val_accuracy: 0.9444\n",
      "Epoch 20/25\n",
      "61/61 [==============================] - 18s 299ms/step - loss: 0.2805 - accuracy: 0.8929 - val_loss: 0.1171 - val_accuracy: 0.9444\n",
      "Epoch 21/25\n",
      "61/61 [==============================] - 17s 286ms/step - loss: 0.2897 - accuracy: 0.8816 - val_loss: 0.0715 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "61/61 [==============================] - 17s 284ms/step - loss: 0.2575 - accuracy: 0.9026 - val_loss: 0.0485 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "61/61 [==============================] - 18s 289ms/step - loss: 0.2376 - accuracy: 0.9098 - val_loss: 0.0550 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "61/61 [==============================] - 18s 287ms/step - loss: 0.2197 - accuracy: 0.9170 - val_loss: 0.0547 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "61/61 [==============================] - 17s 286ms/step - loss: 0.2259 - accuracy: 0.9129 - val_loss: 0.0194 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6c5e3026d8>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 3 - Training the CNN\n",
    "\n",
    "# Compiling the CNN\n",
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Training the CNN on the Training set and evaluating it on the Test set\n",
    "cnn.fit(x = training_set, validation_data = test_set, epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "UlewaZ6-VwWz"
   },
   "outputs": [],
   "source": [
    "# Part 4 - Making a single prediction\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "test_image = image.load_img('/content/drive/My Drive/AML/Cottondisease/data/test/diseased cotton plant/dd (41).jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image=test_image/255\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "aibe2KPUVwW5",
    "outputId": "93504640-5877-4902-934b-f1f4f79607af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.0346232e-07, 9.9528331e-01, 2.0338618e-08, 4.7163931e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title Default title text\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "uFQy6pqVbnux",
    "outputId": "f4b26286-947f-4480-cf5d-ce79b48d7426"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = np.argmax(result)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "XpbZHCvcbthh",
    "outputId": "1aa26170-c15f-437e-d1ab-ba8ffc2439c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diseased cotton plant\n"
     ]
    }
   ],
   "source": [
    "if(pred == 0):\n",
    "  print(\"diseased cotton leaf\")\n",
    "elif(pred == 1):\n",
    "  print(\"diseased cotton plant\")\n",
    "elif(pred == 2):\n",
    "  print(\"fresh cotton leaf\") \n",
    "else:\n",
    "  print(\"fresh cotton plant\") \n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Image Classification_cotton.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
